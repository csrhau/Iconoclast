\section{Power Profiling}
\label{sec:profiling}

We intend to answer two related questions with the practical component of our work. The first of these concerns placing bounds on how much the power consumption of arbitrary code can vary by. By doing so we derive an upper limit of how much it is possible to optimize any code by. These bounds also help us tackle a second issue - namely to provide a fairer appraisal of model accuracies than those often found in the literature.

When optimizing software for reduced power consumption, the only runtime component a software engineer has any influence over is its activity factor. As previously discussed, the activity factor of a processor has a linear relationship with dynamic power and secondary relationships with both static and dynamic power through the actions of DVFS. A developer wishing to save energy should therefore aim to modify their code to reduce its activity factor.

The relationship between code composition and activity factor is a complex one. Recall that dynamic power is drawn as logic elements switch states. Although some of these elements are used to carry out program logic, many more are dedicated to secondary tasks like instruction decoding or propagating clock signals. The latter proves particularly onerous, as by definition the circuitry driven by a clock signal has an activity factor of 1 as it changes state each cycle.\findref


\todo{We consider the accessible dynamic range}
\begin{equation}
0 < \alpha < \beta < 1
\end{equation}

\fragment{We feel it is disingenuous to speed up a code by a large factor and then claim to have optimized for energy. }

\todo{the definitions matter}





There 
Even if the developer had sufficiently intimate knowledge of a processor to judge how much of a chip was involved with different choices of implementation, 



\todo{clock signals}

Although some logic elements are employed to perform 

\todo{From equations}


 efforts of a software engineer focus 


\fragment{
This means that the effect any particular software has on power consumption is bounded by the contribution of $P_{dyn}$ to $P_{tot}$. Furthermore, from the point of view of a performance engineer we need only concern ourselves with understanding and minimizing $P_{dyn}$.
 
}


\todo{Occam's razor}

\fragment{Often power models are assessed by their performance relative to a measured baseline. Although this is important, this figure is somewhat meaningless without context. Our approach is one of quantifying 

We first construct a power model in the vein of those proposed in the literature. We then try to link the predictions of our model to the power equations given previously. Essentially we are attempting to test against the null hypothesis - to show how much better or worse our regressed model is better than a naive attempt.


\begin{figure}
\label{fig:tlpilp}
\includegraphics[width=0.9\linewidth]{./Plots/tlp_vs_ilp/tlpilp-figure0.pdf}
\caption{Power Overhead from TLP and ILP}
\end{figure}


\todo{Make this bit less attacky:}
It is also worth noting that this model is effectively useless from a code optimization standpoint as it does not take any software features into account. This property is intentional, as it allows us once again to provide a baseline from which to assess any models. One can only realistically expect to optimize a code to the level at which any changes can be accurately measured. A model which claims to assist in the optimization of codes can only offer optimizations to the extent it shows divergence from this baseline.

}
\fragment{The unknown quantities in our simplified power equations can be empirically measured.}
Baseline against which to compare the work of others. 


\fragment{Accuracy figures without context are notoriously unreliable. To compensate for this we compare the outputs of various models against a baseline we have devised. This baseline consists of what we regard as the simplest non-trivial power model conceivable. This model stands in as a sort of null hypothesis test, our justification being that a complex model only adds value to the extent with which it outperforms this toy model.}

\fragment{Our toy model is not the simplest model possible - It is well established and readily apparent that runtime is the largest contributory factor to power consumption. One could therefore imagine a simple power model}

\fragment{We consider this to be the absolute minimum power consumption possible.}

\fragment{Intentionally simplistic. Our decision to simplify activity factor to active cores is part of this. We assume that instruction pipe-lining does a reasonable job of keeping as much silicon active as possible, and we do not know how much area an individual instruction activates, and a large percentage of power use is from clock circuitry anyway}



\fragment{Two components to our investigation. Firstly, the upper bound imposed by the baseline power consumption. Secondly, as we can only view power figures approximately, the error introduced into these models necessarily limits their usefulness as optimization tools beyond a certain point.}



\begin{table}
\centering
\small
\begin{tabular}{@{}ccccc@{}} \toprule
&\multicolumn{4}{c}{CPU Cores Active} \\ \cmidrule(r){2-5}
Frequency (GHz) & 1 & 2 & 3 & 4 \\ \midrule 
1.60 & 9.180 & 10.970 & 12.832 & 14.555 \\ 
1.70 & 9.449 & 11.446 & 13.295 & 15.112 \\ 
1.80 & 9.592 & 11.654 & 13.617 & 15.682 \\ 
1.90 & 9.816 & 12.009 & 14.168 & 16.291 \\ 
2.10 & 10.272 & 12.709 & 15.161 & 17.605 \\ 
2.20 & 10.559 & 13.161 & 15.705 & 18.333 \\ 
2.30 & 10.812 & 13.551 & 16.419 & 19.070 \\ 
2.40 & 11.303 & 14.290 & 17.012 & 19.946 \\ 
2.50 & 11.680 & 14.784 & 18.000 & 20.837 \\ 
2.60 & 11.819 & 15.144 & 18.616 & 21.879 \\ 
2.70 & 12.205 & 15.830 & 19.379 & 22.940 \\ 
2.90 & 13.095 & 17.196 & 21.155 & 25.344 \\ 
3.00 & 13.547 & 18.160 & 22.210 & 26.759 \\ 
3.10 & 14.048 & 18.870 & 23.639 & 28.284 \\ 
3.20 & 14.504 & 19.726 & 24.940 & 29.857 \\ 
\bottomrule
\end{tabular}
   \vspace{0.5\baselineskip}
\caption{Test Platform Base CPU Power (W)}
\end{table} 



\todo{table 2 - benchmark results. Linpack is 35.2428 for 100 seconds}
\todo{Something about how any delta is about how the code uses more logic elements than our NOP lo ops. Our optimization window is therefore within that Delta}

\todo{Be nice, say that this does not invalidate other work, but simply shows that hardware has now reached \reword{convergence} and basically there's little traction left}

Having shown that 

\fragment{This model is not supposed to be rigorous or precise. Rather we present it as the simplest possible non-trivial power model which accounts for the sources of variability in the power equations. In effect our model is functionally equivalent to the power equations
presented previously with appropriate constants substituted \todo{sampled}. We present this model as our null hypothesis - for a model to be useful it must outperform this one. It is necessarily an oversimplification - it ignores clock gating. It should also consistently underestimate the true value, as the benchmark selected intentionally exercises the minimum possible number of logic elements while still performing work.}


\todo{One limitation of this work stems from the nature of RAPL - it is an accumulative measurement of power taking into account all processes. On a loaded system it will measure all tasks }



\todo{Decompose the model - find baseline vs non-baseline components}
\todo{This kind of shows us that the baseline dominates}

\fragment{Limitations to optimizations - the superfluous and the logical equivalences. The first is a no-brainer and boils down to removing unnecessary pre-fetching}

\fragment{Either optimizations which are off the critical path, or else those which are on the critical path }

\fragment{Put another way, any optimization which trades runtime for power has a limited window of}

\todo{equationify fact that energy is power * time, and assume we have power decreases, time static or increases}
\todo{equationify baseline lt optimized lt unoptimized lt roofline} \todo{note - roofline is tdp max}

\todo{Do maths think - by what margin would power have to go down to justify longer runtime? ratio of cost per watt, amortized cost per second}

Nothing discussed so far precludes power optimization in practice.
\todo{imply limits thus far are theoretical}
Even these tight limits may still admit some benefits at extreme scale.
Our final argument however is strictly economic.
\reword{A great deal of attention is paid to the fact that power costs are approaching parity with machine construction costs.} The \choice{implicit, unspoken} \choice{consequence, corollary, implication} being that this has not yet happened. \todo{Ultimate point being here the price difference, machine vs power cost places a further limit on optimization utility. Even if we manage to find a slower, more power efficient method of computing a given result, the cost of energy saved has to be less than the added amortized runtime cost.}


\todo{Legitimate targets for optimization: removing redundant prefetch operations as per phi paper.}
