/* ----------------------------------------------------------------------
   miniMD is a simple, parallel molecular dynamics (MD) code.   miniMD is
   an MD microapplication in the Mantevo project at Sandia National 
   Laboratories ( http://www.mantevo.org ). The primary 
   authors of miniMD are Steve Plimpton, Paul Crozier (pscrozi@sandia.gov)
   and Christian Trott (crtrott@sandia.gov).

   Copyright (2008) Sandia Corporation.  Under the terms of Contract
   DE-AC04-94AL85000 with Sandia Corporation, the U.S. Government retains
   certain rights in this software.  This library is free software; you 
   can redistribute it and/or modify it under the terms of the GNU Lesser 
   General Public License as published by the Free Software Foundation; 
   either version 3 of the License, or (at your option) any later 
   version.
  
   This library is distributed in the hope that it will be useful, but
   WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
   Lesser General Public License for more details.
    
   You should have received a copy of the GNU Lesser General Public 
   License along with this software; if not, write to the Free Software
   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307
   USA.  See also: http://www.gnu.org/licenses/lgpl.txt .

   For questions, contact Paul S. Crozier (pscrozi@sandia.gov) or
   Christian Trott (crtrott@sandia.gov). 

   Please read the accompanying README and LICENSE files.
---------------------------------------------------------------------- */


------------------------------------------------
Description:
------------------------------------------------
miniMD is a parallel molecular dynamics (MD) simulation package written
in C++ and intended for use on parallel supercomputers and new 
architechtures for testing purposes. The software package is meant to  
be simple, lightweight, and easily adapted to new hardware. It is 
designed following many of the same algorithm concepts as our LAMMPS 
(http://lammps.sandia.gov) parallel MD code, but is much simpler.

Authors: Steve Plimpton, Paul Crozier (pscrozi@sandia.gov)
   and Christian Trott (crtrott@sandia.gov)        

This simple code is a self-contained piece of C++ software 
that performs parallel molecular dynamics simulation of a Lennard-Jones
or a EAM system and gives timing information.

It is implemented to be very scalable (in a weak sense).  Any 
reasonable parallel computer should be able to achieve excellent 
scaled speedup (weak scaling).  miniMD uses a spatial decomposition
parallelism and has many other similarities to the much more 
complicated LAMMPS MD code: http://lammps.sandia.gov

The sub-directories contain different variants of miniMD:

miniMD_ref:         supports MPI+OpenMPI hybrid mode.
miniMD_OpenCL:      an OpenCL version of miniMD, uses MPI to parallelize over 
                    multiple devices. Limited Features.
miniMD_KokkosArray: supports MPI and uses KokkosArray on top of it, compiles
                    with pThreads or CUDA backend

Each variant is self contained and does not reference any source files of 
the other variants.

 
------------------------------------------------
Strengths and Weaknesses:
------------------------------------------------

miniMD consists of less than 5,000 lines of C++ code. Like LAMMPS, miniMD uses
spatial decomposition MD, where individual processors in a cluster own subsets
of the simulation box. And like LAMMPS, miniMD enables users to specify a problem
size, atom density, temperature, timestep size, number of timesteps to perform,
and particle interaction cutoff distance. But compared to LAMMPS, MiniMD's feature 
set is extremely limited, and only two types interactions (Lennard-Jones/ EAM) are
available. No long-range electrostatics or molecular force field features are 
available. Inclusion of such features is unnecessary for testing basic MD and 
would have made miniMD much bigger, more complicated, and harder to port to novel
hardware. The current version of LAMMPS includes over 200,000 lines of code in 
hundreds of files, nineteen optional packages, over one hundred different commands, 
and over five hundred pages of documentation. Such a large and complicated code 
is not ideally suited for answering certain performance questions or for tinkering 
by non-MD-experts.


------------------------------------------------
Compiling the code:
------------------------------------------------

There is a simple Makefile that should be easily modified for most 
Unix-like environments.  There are also one or more Makefiles with 
extensions that indicate the target machine and compilers. Read the 
Makefile for further instructions.  If you generate a Makefile for 
your platform and care to share it, please send it to Paul Crozier:
pscrozi@sandia.gov . By default the code compiles with MPI support 
and can be run on one or more processors.  

==Compiling:

  make <platform>

  For the KokkosArray variant, you first need to go to the 'common' directory
  and execute the appropriate build script there. Note, when building 
  directly out of the svn repository you need to do 

  make <platform> svn=1   

  for building miniMD_KokkosArray

  Furthermore miniMD_reference and miniMD_KokkosArray support both single
  and double precision builds. Those can be triggered by using
  -DPRECISION=1 and -DPRECISION=2 as compiler flags in the Makefiles.

==To remove all output files, type:

  make clean_<platform>

  or 

  make clean

==Testing:

  make test

  The test will run a simulation and compare it against reference output. 
  Where are different test modes, which change the amount of tests run.
  Running 'make test' will give instructions how to run more complex tests.
  Note the test does not currently run with multiple GPUs since it does not 
  provide the necessary environment variables.

------------------------------------------------
Running the code and sample I/O:
------------------------------------------------

Usage:

miniMD (serial mode)

mpirun -np numproc miniMD (MPI mode)

Example:

mpirun -np 16 ./miniMD 

MiniMD understands a number of command-line options. To get the options 
for each particular variant of miniMD please use "-h" as an argument.

You will also need to provide a simple input script, which you can model
after the ones included in this directory (e.g. in.lj.miniMD). The format and
parameter description is as follows:

Sample input file contents found in "lj.in":
------------------------------------------------

Lennard-Jones input file for MD benchmark

lj             units (lj or metal)
none           data file (none or filename)       
lj             force style (lj or eam)
32 32 32       size of problem
100            timesteps
0.005          timestep size 
1.44           initial temperature 
0.8442         density 
20             reneighboring every this many steps
2.5 0.30       force cutoff and neighbor skin 
100            thermo calculation every this many steps (0 = start,end)


------------------------------------------------

Sample output file contents found in "out.lj.miniMD":
------------------------------------------------

Create System:
Done .... 
# miniMD-KokkosArray-ParallelFor  (MPI+PThreads or MPI+CUDA via Kokkos) output ...
# Systemparameters: 
  # MPI processes: 2
  # OpenMP threads: 1
  # Inputfile: in.lj.miniMD
  # Datafile: None
  # ForceStyle: LJ
  # Units: LJ
  # Atoms: 131072
  # System size: 53.75 53.75 53.75 (unit cells: 32 32 32)
  # Density: 0.844200
  # Force cutoff: 2.500000
  # Neigh cutoff: 2.800000
  # Half neighborlists: 0
  # Neighbor bins: 14 14 14
  # Neighbor frequency: 20
  # Timestep size: 0.005000
  # Thermo frequency: 100
  # Ghost Newton: 1
  # Use SSE intrinsics: 0
  # Do safe exchange: 0
  # Size of float: 4

# Starting dynamics ...
# Timestep T U P Time
0 1.440000e+00 -6.773366e+00 -5.019680e+00  0.000
100 8.200911e-01 -5.852703e+00 -1.873927e-01  0.569


# Performance Summary:
# MPI_proc OMP_threads nsteps natoms t_total t_force t_neigh t_comm t_other performance perf/thread grep_string t_extra
2 1 100 131072 0.569370 0.191551 0.095072 0.213666 0.069081 23020520.557160 11510260.278580 PERF_SUMMARY 0.091989


------------------------------------------------
Running on GPUs with KokkosArray
------------------------------------------------

The KokkosArray variant needs a CUDA aware MPI for running on GPUs (though it might work on a single GPU without any MPI).
Currently known MPI implementations with CUDA support are:
mvapich2 1.8 or higher
openmpi 1.7 or higher
cray mpi on XK6 and higher

Note those typically require some environment variables to be set. For example mvapich2 1.9 can be used like this:
mpiexec -np 2 -env MV2_USE_CUDA=1 ./miniMD_mvapichcuda --half_neigh 0 -s 60


------------------------------------------------

Known Issues:
------------------------------------------------


The OpenCL variant does not currently support all features of the Reference and KokkosArray variant. In particular
it does not support EAM simulations. Also due to limitations in OpenCL (and the author not having the time to work
around them) the simulations are limited to about 240k atoms in the standard LJ settings. This corresponds to -s 39.

Running the in.*-data.miniMD inputs on the GPU with the KokkosArray variant defaults to too many neighbor bins. This
causes significantly increased memory consumption and longer runtimes. Use -b 30 as a command line option, to override
the default neighbor bin size.

The option --safe_exchange is currently not active. 
